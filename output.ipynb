{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# load yolov8 model\n",
    "model = YOLO('best.pt')\n",
    "#.cuda()\n",
    "# load video\n",
    "video_path = 'input.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "video_out_path = os.path.join('.', 'out.mp4')\n",
    "\n",
    "ret, frame = cap.read()\n",
    "size = (int(cap.get(3)), int(cap.get(4)))  # Get the size from the video capture\n",
    "cap_out = cv2.VideoWriter(video_out_path,  \n",
    "                         cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                         cap.get(cv2.CAP_PROP_FPS), size) \n",
    "ret = True\n",
    "# read frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.DataFrame(columns=['wood','glass','plastic','metal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while ret:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # detect objects\n",
    "        # track objects\n",
    "        results = model.track(frame, persist=True)\n",
    "\n",
    "        # plot results\n",
    "        # cv2.rectangle\n",
    "        # cv2.putText\n",
    "        frame_ = results[0].plot()\n",
    "        dice = {}\n",
    "        o = {}\n",
    "        for i in results[0].boxes.cls:\n",
    "            if not int(i) in dice.keys():\n",
    "                dice[int(i)]=1\n",
    "            else:\n",
    "                dice[int(i)]+=1\n",
    "        # visualize\n",
    "        cv2.imshow('frame', frame_)\n",
    "        o['wood'] = dice[0] if (0 in dice.keys()) else 0\n",
    "        o['glass'] = dice[1] if (1 in dice.keys()) else 0\n",
    "        o['plastic'] = dice[2] if (2 in dice.keys()) else 0\n",
    "        o['metal'] = dice[3] if (3 in dice.keys()) else 0\n",
    "\n",
    "        csv = pd.concat([csv, pd.DataFrame([o])], ignore_index=True)\n",
    "        print(csv)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        cap_out.write(frame_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'class_0', 1: 'class_1', 2: 'class_2', 3: 'class_3'}\n",
      "orig_img: array([[[122, 124, 126],\n",
      "        [122, 124, 126],\n",
      "        [122, 124, 126],\n",
      "        ...,\n",
      "        [156, 166, 165],\n",
      "        [156, 166, 165],\n",
      "        [156, 166, 165]],\n",
      "\n",
      "       [[122, 124, 126],\n",
      "        [122, 124, 126],\n",
      "        [122, 124, 126],\n",
      "        ...,\n",
      "        [152, 162, 161],\n",
      "        [152, 162, 161],\n",
      "        [152, 162, 161]],\n",
      "\n",
      "       [[120, 122, 124],\n",
      "        [120, 122, 124],\n",
      "        [120, 122, 124],\n",
      "        ...,\n",
      "        [151, 156, 156],\n",
      "        [149, 154, 154],\n",
      "        [149, 154, 154]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 68,  78,  82],\n",
      "        [ 68,  78,  82],\n",
      "        [ 68,  78,  82],\n",
      "        ...,\n",
      "        [144, 145, 140],\n",
      "        [144, 145, 140],\n",
      "        [144, 145, 140]],\n",
      "\n",
      "       [[ 76,  88,  93],\n",
      "        [ 76,  88,  93],\n",
      "        [ 77,  89,  94],\n",
      "        ...,\n",
      "        [149, 150, 145],\n",
      "        [149, 150, 145],\n",
      "        [149, 150, 145]],\n",
      "\n",
      "       [[ 76,  88,  93],\n",
      "        [ 76,  88,  93],\n",
      "        [ 77,  89,  94],\n",
      "        ...,\n",
      "        [144, 145, 140],\n",
      "        [144, 145, 140],\n",
      "        [144, 145, 140]]], dtype=uint8)\n",
      "orig_shape: (360, 640)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': 1.4889240264892578, 'inference': 1087.7506732940674, 'postprocess': 0.9930133819580078}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.track(video_path, show=True, stream=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "video 1/1 (1/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 2 class_3s, 1143.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 2., 1., 1., 3., 3.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (2/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 2 class_3s, 1071.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 2., 1., 1., 3., 3.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (3/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 2 class_3s, 1066.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 2., 1., 1., 3., 3.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (4/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 4 class_2s, 2 class_3s, 1124.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 2., 1., 1., 3., 3., 1.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (5/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 3 class_2s, 2 class_3s, 1104.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 1., 1., 3., 3., 1.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (6/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 3 class_2s, 2 class_3s, 1154.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 1., 1., 3., 3., 1.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (7/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 3 class_2s, 3 class_3s, 1137.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 1., 1., 3., 3., 1., 1., 3.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11., 12., 13.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (8/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 8 class_1s, 4 class_2s, 3 class_3s, 1073.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 1., 1., 3., 3., 1., 1., 3., 1., 1., 2.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (9/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 8 class_1s, 4 class_2s, 4 class_3s, 1067.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 2., 1., 2., 1., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3.])\n",
      "tensor([ 1.,  2.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (10/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 8 class_1s, 3 class_2s, 4 class_3s, 1074.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (11/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 3 class_2s, 6 class_3s, 1051.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (12/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 3 class_2s, 6 class_3s, 1082.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (13/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 6 class_3s, 1110.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (14/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 7 class_3s, 1074.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (15/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 7 class_3s, 1060.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (16/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 7 class_3s, 1095.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (17/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 7 class_3s, 1064.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (18/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 4 class_2s, 9 class_3s, 1090.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 1., 3., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  7.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (19/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 4 class_2s, 8 class_3s, 1076.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  9., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (20/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 4 class_2s, 8 class_3s, 1176.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  9., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (21/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 5 class_2s, 8 class_3s, 1072.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  9., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (22/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 7 class_1s, 5 class_2s, 8 class_3s, 1074.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 2., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1.])\n",
      "tensor([ 1.,  3.,  4.,  6.,  9., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (23/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 5 class_2s, 8 class_3s, 1132.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([ 1.,  6.,  9., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (24/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 6 class_1s, 5 class_2s, 7 class_3s, 1083.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([ 1.,  6., 11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (25/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 4 class_2s, 7 class_3s, 1096.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (26/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 4 class_2s, 7 class_3s, 1048.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (27/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 5 class_1s, 4 class_2s, 7 class_3s, 1057.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([11., 12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (28/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 7 class_3s, 1076.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2.])\n",
      "tensor([12., 13., 14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (29/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 7 class_3s, 1058.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2., 1., 3.])\n",
      "tensor([14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31., 33., 34.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (30/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 4 class_2s, 7 class_3s, 1068.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 2., 3., 3., 3., 2., 3., 3., 3., 2., 1., 2., 1., 3.])\n",
      "tensor([14., 15., 16., 17., 18., 19., 22., 23., 26., 27., 29., 30., 31., 33., 34.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (31/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 3 class_2s, 6 class_3s, 1066.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 3., 2., 3., 3., 3., 2., 1., 2., 1., 3., 1.])\n",
      "tensor([14., 18., 19., 22., 23., 26., 27., 29., 30., 31., 33., 34., 37.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (32/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 3 class_2s, 7 class_3s, 1092.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 2., 3., 3., 3., 2., 1., 2., 1., 3., 1., 3.])\n",
      "tensor([18., 19., 22., 23., 26., 27., 29., 30., 31., 33., 34., 37., 38.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (33/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 4 class_2s, 5 class_3s, 1117.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 3., 3., 2., 1., 2., 1., 3., 1., 3., 2.])\n",
      "tensor([22., 23., 26., 27., 29., 30., 31., 33., 34., 37., 38., 41.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (34/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 4 class_2s, 6 class_3s, 1122.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3.])\n",
      "tensor([22., 23., 26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (35/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 4 class_2s, 6 class_3s, 1103.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3.])\n",
      "tensor([22., 23., 26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (36/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 4 class_2s, 5 class_3s, 1066.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3.])\n",
      "tensor([22., 26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (37/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 3 class_2s, 6 class_3s, 1100.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3., 3.])\n",
      "tensor([26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42., 43.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (38/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 3 class_2s, 6 class_3s, 1092.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3., 3.])\n",
      "tensor([26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42., 43.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (39/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 3 class_2s, 6 class_3s, 1059.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 2., 1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([26., 27., 29., 30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (40/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 3 class_2s, 4 class_3s, 1090.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([29., 30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (41/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 3 class_2s, 4 class_3s, 1104.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([29., 30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (42/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 3 class_2s, 4 class_3s, 1102.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([29., 30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (43/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 2 class_2s, 4 class_3s, 1097.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (44/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 2 class_2s, 4 class_3s, 1094.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (45/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 4 class_1s, 2 class_2s, 4 class_3s, 1089.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([30., 31., 33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (46/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 1 class_2, 4 class_3s, 1082.8ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (47/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 1 class_2, 4 class_3s, 1067.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (48/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 1 class_2, 4 class_3s, 1092.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 3., 2., 3., 3., 1.])\n",
      "tensor([33., 34., 37., 38., 41., 42., 43., 47.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (49/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 2 class_2s, 4 class_3s, 1116.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 3., 2., 3., 3., 1., 2.])\n",
      "tensor([33., 34., 37., 38., 41., 42., 43., 47., 52.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (50/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 2 class_2s, 4 class_3s, 1119.0ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 1., 3., 2., 3., 3., 1., 2.])\n",
      "tensor([33., 34., 37., 38., 41., 42., 43., 47., 52.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "video 1/1 (51/335) d:\\object-tracking-yolov8-native-main\\input.mp4: 384x640 3 class_1s, 2 class_2s, 4 class_3s, 1109.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 3., 2., 3., 3., 1., 2., 3.])\n",
      "tensor([33., 37., 38., 41., 42., 43., 47., 52., 53.])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\object-tracking-yolov8-native-main\\rensfie.ipynb Ячейка 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/object-tracking-yolov8-native-main/rensfie.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m results:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/object-tracking-yolov8-native-main/rensfie.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(i\u001b[39m.\u001b[39;49mboxes\u001b[39m.\u001b[39;49mcls)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/object-tracking-yolov8-native-main/rensfie.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(i\u001b[39m.\u001b[39;49mboxes\u001b[39m.\u001b[39;49mid)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\utils\\_contextlib.py:56\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m             \u001b[39m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[0;32m     55\u001b[0m             \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 56\u001b[0m                 response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39msend(request)\n\u001b[0;32m     58\u001b[0m \u001b[39m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[39m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[1;32m--> 259\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(im, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    261\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:135\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m visualize \u001b[39m=\u001b[39m increment_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir \u001b[39m/\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstem,\n\u001b[0;32m    134\u001b[0m                            mkdir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mvisualize \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type\u001b[39m.\u001b[39mtensor) \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:347\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    344\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[0;32m    348\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[0;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 59\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 79\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m     80\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:40\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32md:\\Python 3.11\\Lib\\site-packages\\torch\\nn\\functional.py:2058\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2056\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, inplace\u001b[39m=\u001b[39minplace)\n\u001b[0;32m   2057\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2058\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu_(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   2059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu(\u001b[39minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. Проверьте код в ячейках, чтобы определить возможную причину сбоя. Щелкните <a href=\"https://aka.ms/vscodeJupyterKernelCrash\">здесь</a> для получения дополнительных сведений. Подробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "for i in results:\n",
    "    print(i.boxes.cls)\n",
    "    print(i.boxes.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
